---
title: "Lab 6: Machine Learning in Hydrology"
author: "Erica. Mortland"
date: "2025-04-04"
format: html
execute: 
  echo: true
---

## Lab Set Up

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
```

## Data Download

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

## Getting the Documentation PDF

```{r}
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf', mode = "wb")
```

## Getting Basin Characteristics

```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
```

```{r}
remote_files  <- glue('{root}/camels_{types}.txt')

local_files   <- glue('data/camels_{types}.txt')
```

```{r}
walk2(remote_files, local_files, download.file, quiet = TRUE)
```

```{r}
camels <- map(local_files, read_delim, show_col_types = FALSE) 
```

```{r}
camels <- power_full_join(camels ,by = 'gauge_id')
```

# Question 1:

From the documentation PDF, *zero_q_freq* represents the frequency of days where streamflow (Q) is 0 mm per day, meaning no measurable water flow was recorded.

## Exploratory Data Analysis:

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```

# Question 2:

```{r}
library(ggplot2)
library(ggthemes)
library(patchwork)
library(sf)
library(viridis)
```

```{r}
camels_sf <- st_as_sf(camels, coords = c("gauge_lon", "gauge_lat"), crs = 4326)

map_aridity <- ggplot(data = camels_sf) +
  geom_sf(aes(color = aridity), size = 2) +
  scale_color_viridis(option = "C", name = "Aridity") +
  labs(title = "Sites Colored by Aridity", x = "Longitude", y = "Latitude") +
  theme_minimal()

map_p_mean <- ggplot(data = camels_sf) +
  geom_sf(aes(color = p_mean), size = 2) +
  scale_color_viridis(option = "D", name = "Mean Precipitation (mm)") +
  labs(title = "Sites Colored by Mean Precipitation", x = "Longitude", y = "Latitude") +
  theme_minimal()

combined_map <- map_aridity + map_p_mean + plot_layout(ncol = 1)

print(combined_map)
```

## Model preparation:

```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()
```

## Visual EDA:

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  scale_color_viridis_c() +
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

## Testing a transformation:

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

## Log transform color scale

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

## Splitting the data:

```{r}
set.seed(99)

camels <- camels |> 
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

## Create a recipe to preprocess data:

```{r}
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) |> 
  step_naomit(all_predictors(), all_outcomes())
```

## Native base *lm* approach:

```{r}
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)
```

```{r}
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

## Statistical and Visual Evaluation:

```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```

```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  scale_color_gradient2(low = "red", mid = "orange", high = "yellow") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

```{r}
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train) 

summary(extract_fit_engine(lm_wf))$coefficients
```

```{r}
summary(lm_base)$coefficients
```

## Making predictions:

```{r}
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

## Statistical and Visual Evaluation:

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
library(baguette)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train) 
```

## Predictions:

```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

## Statistical and Visual Evaluation:

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

## A workflowset approach:

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

# Question 3:

## Build *xgboost* regression model:

```{r}
library(xgboost)

xgb_model <- boost_tree(trees = 1000, tree_depth = 6, learn_rate = 0.1) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(xgb_model) %>%
  fit(data = camels_train)
```

```{r}
xgb_data <- augment(xgb_wf, new_data = camels_test)

ggplot(xgb_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

## Build a neural network model:

```{r}
nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

nn_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(nn_model) %>%
  fit(data = camels_train)
```

## Evaluate the model:

```{r}
nn_data <- augment(nn_wf, new_data = camels_test)

ggplot(nn_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

## Comparing all models:

```{r}
xg_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xg_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(xg_model) %>%
  fit(data = camels_train)

nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

nn_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(nn_model) %>%
  fit(data = camels_train)


lm_data <- augment(lm_wf, new_data = camels_test)

rf_data <- augment(rf_wf, new_data = camels_test)

xgb_data <- augment(xgb_wf, new_data = camels_test)

nn_data <- augment(nn_wf, new_data = camels_test)

```

## Evaluate Model Performance:
```{r}
library(yardstick)

evaluate_model <- function(data) {
  metrics(data, truth = logQmean, estimate = .pred)
}

lm_metrics <- evaluate_model(lm_data) %>% mutate(model = "Linear Model")

rf_metrics <- evaluate_model(rf_data) %>% mutate(model = "Random Forest")

xgb_metrics <- evaluate_model(xgb_data) %>% mutate(model = "XGBoost")

nn_metrics <- evaluate_model(nn_data) %>% mutate(model = "Neural Network")

model_comparison <- bind_rows(lm_metrics, rf_metrics, xgb_metrics, nn_metrics)

print(model_comparison)
```

## Visual Evaluation:
```{r}
ggplot(model_comparison, aes(x = model, y = .estimate, fill = .metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Model Performance Comparison", y = "Metric Value", x = "Model")
```

## Which of the 4 models would you move forward with?

When comparing all three evaluation metrics (RMSE, R², and MAE), the neural network model consistently outperforms the others, showing the highest overall accuracy. I would recommend moving forward with this model because it has the lowest RMSE (indicating more precise predictions), the highest R² (suggesting a better fit to the data), and the lowest MAE (showing less overall prediction error). While the improvements are not massive, the consistent advantage across all metrics makes the neural network the strongest and most reliable choice among the four models.

# Build Your Own

## Data Splitting:

```{r}

set.seed(99)

split_data <- initial_split(camels, prop = 0.75)

train_data <- training(split_data)
test_data <- testing(split_data)

cv_splits <- vfold_cv(train_data, v = 10)
```

## Recipe:

```{r}
formula <- logQmean ~ p_mean + pet_mean + elev_mean + area_gages2 + max_water_content + slope_mean
```

### Describe why you're choosing this formula:

I chose this formula because it includes key factors that affect streamflow, like precipitation, evapotranspiration, elevation, slope, area, and soil water content. These variables influence how much water enters, moves through, and is stored in a watershed. Understanding these drivers is important for predicting water availability and managing water resources effectively.

```{r}
rec <- recipe(formula, data = camels_train) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors())
```

## Define 3 models:

### Random forest model:
```{r}
library(parsnip)

rf_camel <- rand_forest(trees = 1000, mtry = 4, min_n = 5) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

### XGBoost model:
```{r}
xgb_camel <- boost_tree(trees = 1000, tree_depth = 6, learn_rate = 0.1) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

### Neural network model:
```{r}
nn_camel <- bag_mlp(hidden_units = 3, penalty = 0.01) %>%
  set_engine("nnet") %>%
  set_mode("regression")
```

## Workflow set()
```{r}

wf_2 <- workflow_set(list(rec), list(xgb_camel, rf_camel, nn_camel)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)
```

## Evaluation:
```{r}
autoplot(wf_2)
```

```{r}
ranked_results <- wf_2 %>%
  rank_results()

print(ranked_results)
```

The best model from this set is the neural network, as it’s the only one with an R² value above 0.9—meeting the lab’s success threshold. It also has the lowest RMSE, meaning its predictions are more accurate than those from the boosted tree or random forest models.

## Extract and Evaluate:

```{r}
nn_wf <- workflow() %>%
  add_model(nn_camel) %>%
  add_recipe(rec)

nn_fit <- fit(nn_wf, data = train_data)

library(broom)
nn_preds <- augment(nn_fit, new_data = test_data)

ggplot(nn_preds, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = .pred), alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
  scale_color_viridis_c() +
  labs(
    title = "Neural Network Model: Predicted vs. Observed logQmean",
    x = "Predicted logQmean",
    y = "Observed logQmean",
    color = "Predicted"
  ) +
  theme_minimal()
```
